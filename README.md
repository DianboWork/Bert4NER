In this work,  I used the [BERT](https://arxiv.org/abs/1810.04805) as the encoder to solve the English Named Entity Recognition (NER). 

What's the difference between this work and previous work? For BERT, it uses wordpiece tokenization, which means one word may break into several pieces (sub-word units). Then for NER, how to find the corresponding class label for these sub-word units is a problem. There are some discussions, such as  https://github.com/google-research/bert/issues/560 https://github.com/huggingface/transformers/issues/323 and https://github.com/huggingface/transformers/issues/64.  In this work, the average pooling over sub-word units (shown in https://github.com/DianboWork/Bert4NER/blob/5a0278898226688f317341a1c2f0d7fc093761d8/models/pretrain_base.py#L37) is used to get the word representation, which is a more natural way and avoids the introduction of  the new tag ("X").
